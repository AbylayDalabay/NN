{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание\n",
    "\n",
    "1. Мы будем работать с (частичными) данными lenta.ru отсюда: https://www.kaggle.com/yutkin/corpus-of-russian-news-articles-from-lenta/\n",
    "2. Проведите препроцессинг текста. Разбейте данные на train и test для задачи классификации (в качестве метки класса будем использовать поле topic). В качестве данных для классификации в пунктах 3 и 5 возьмите\n",
    "    - только заголовки (title)\n",
    "    - только тексты новости (text)\n",
    "    - и то, и другое\n",
    "3. Обучите fastText для классификации текстов по темам. Сравните качество для разных данных из п. 2.\n",
    "4. Обучите свою модель w2v (или возьмите любую подходящую предобученную модель). Реализуйте функцию для вычисления вектора текста / заголовка / текста+заголовка как среднего вектора входящих в него слов. \n",
    "     - (Бонус) Модифицируйте функцию вычисления среднего вектора: взвешивайте вектора слов соответствующими весами tf-idf.\n",
    "5. Обучите на полученных средних векторах алгоритм классификации, сравните полученное качество с классификатором fastText. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data/corpus-of-russian-news-articles-from-lenta.zip\n",
      "  inflating: data/lenta-ru-news.csv  \n"
     ]
    }
   ],
   "source": [
    "# !kaggle datasets download -d yutkin/corpus-of-russian-news-articles-from-lenta\n",
    "!unzip data/corpus-of-russian-news-articles-from-lenta.zip -d data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22744/352426645.py:3: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  lenta = pd.read_csv(data_path + 'lenta-ru-news.csv', usecols=['title', 'text', 'topic'])\n"
     ]
    }
   ],
   "source": [
    "data_path = 'data/'\n",
    "\n",
    "lenta = pd.read_csv(data_path + 'lenta-ru-news.csv', usecols=['title', 'text', 'topic'])\n",
    "lenta = lenta[lenta['topic'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(738973, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Библиотека': 0,\n",
       " 'Россия': 1,\n",
       " 'Мир': 2,\n",
       " 'Экономика': 3,\n",
       " 'Интернет и СМИ': 4,\n",
       " 'Спорт': 5,\n",
       " 'Культура': 6,\n",
       " 'Из жизни': 7,\n",
       " 'Силовые структуры': 8,\n",
       " 'Наука и техника': 9,\n",
       " 'Бывший СССР': 10,\n",
       " 'Дом': 11,\n",
       " 'Сочи': 12,\n",
       " 'ЧМ-2014': 13,\n",
       " 'Путешествия': 14,\n",
       " 'Ценности': 15,\n",
       " 'Легпром': 16,\n",
       " 'Бизнес': 17,\n",
       " 'МедНовости': 18,\n",
       " 'Оружие': 19,\n",
       " '69-я параллель': 20,\n",
       " 'Культпросвет ': 21,\n",
       " 'Крым': 22}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict = {}\n",
    "\n",
    "for i, topic in enumerate(lenta['topic'].unique()):\n",
    "    label_dict[topic] = i\n",
    "\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk import tokenize\n",
    "\n",
    "# tokenizer = tokenize.NLTKWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# from string import punctuation\n",
    "\n",
    "# noise = stopwords.words('russian') + list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pymorphy3\n",
    "# morph = pymorphy3.MorphAnalyzer()\n",
    "\n",
    "# def morphling_lemmatizer(word):\n",
    "#     parsed_word = morph.parse(word)[0]\n",
    "#     lemma = parsed_word.normal_form\n",
    "\n",
    "#     return lemma\n",
    "\n",
    "# morphling_lemmatizer('деревьев')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem import SnowballStemmer\n",
    "\n",
    "# snowball_stemmer = SnowballStemmer(\"russian\")\n",
    "\n",
    "# def ru_stemmer(word):\n",
    "#     return snowball_stemmer.stem(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(sentence):\n",
    "#     token_sentence = tokenizer.tokenize(sentence)   \n",
    "#     clean_tokens = [token for token in token_sentence if token not in noise]\n",
    "#     #lemma_sentence = [morphling_lemmatizer(token) for token in clean_tokens]  \n",
    "#     stemmed_sentence = [ru_stemmer(token) for token in clean_tokens]\n",
    "#     return stemmed_sentence\n",
    "\n",
    "# preprocess('как купить много деревьев')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# title_preprocessed = [str(sentence).lower() for sentence in tqdm(lenta['title'], desc='Preprocessing titles ...')]\n",
    "# text_preprocessed = [str(sentence).lower() for sentence in tqdm(lenta['text'], desc='Preprocessing text ...')]\n",
    "\n",
    "# preprocessed_lenta = pd.DataFrame({\n",
    "#     'title' : title_preprocessed,\n",
    "#     'text' : text_preprocessed,\n",
    "#     'topic' : lenta['topic']\n",
    "# })\n",
    "\n",
    "# preprocessed_lenta.to_csv('preprocessed_lenta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenta['label'] = lenta['topic'].apply(lambda x: label_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic\n",
       "Россия               160445\n",
       "Мир                  136621\n",
       "Экономика             79528\n",
       "Спорт                 64413\n",
       "Культура              53797\n",
       "Бывший СССР           53402\n",
       "Наука и техника       53136\n",
       "Интернет и СМИ        44663\n",
       "Из жизни              27605\n",
       "Дом                   21734\n",
       "Силовые структуры     19596\n",
       "Ценности               7766\n",
       "Бизнес                 7399\n",
       "Путешествия            6408\n",
       "69-я параллель         1268\n",
       "Крым                    666\n",
       "Культпросвет            340\n",
       "Легпром                 114\n",
       "Библиотека               65\n",
       "Оружие                    3\n",
       "ЧМ-2014                   2\n",
       "Сочи                      1\n",
       "МедНовости                1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenta['topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_test_and_save_train(X_df, y_df, train_path):\n",
    "    X, y = X_df.tolist(), y_df.tolist()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, shuffle=True)\n",
    "\n",
    "    with open(data_path+train_path+'.txt', 'w', encoding='utf-8') as file:\n",
    "        for X_entry, y_entry in zip(X_train, y_train):\n",
    "            X_entry = str(X_entry).replace('\\n', ' ').replace('\\r', ' ')\n",
    "            file.write('__label__' + str(y_entry) + ' ' + X_entry)\n",
    "            file.write('\\n')\n",
    "\n",
    "    return X_test, y_test\n",
    "            \n",
    "X_test, y_test = get_test_and_save_train(lenta['title'], lenta['label'], train_path='lenta_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'fastText'...\n",
      "remote: Enumerating objects: 3998, done.\u001b[K\n",
      "remote: Counting objects: 100% (1026/1026), done.\u001b[K\n",
      "remote: Compressing objects: 100% (195/195), done.\u001b[K\n",
      "remote: Total 3998 (delta 890), reused 859 (delta 826), pack-reused 2972 (from 1)\u001b[K\n",
      "Receiving objects: 100% (3998/3998), 8.30 MiB | 438.00 KiB/s, done.\n",
      "Resolving deltas: 100% (2528/2528), done.\n",
      "Processing ./fastText\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pybind11>=2.2 (from fasttext==0.9.2)\n",
      "  Using cached pybind11-2.13.3-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /home/dalabaya/anaconda3/envs/ml-env/lib/python3.10/site-packages (from fasttext==0.9.2) (72.1.0)\n",
      "Requirement already satisfied: numpy in /home/dalabaya/anaconda3/envs/ml-env/lib/python3.10/site-packages (from fasttext==0.9.2) (1.26.4)\n",
      "Using cached pybind11-2.13.3-py3-none-any.whl (240 kB)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=321714 sha256=b6baf2fa882ffdb4c672fa7d55f531b6fddb9321b98bf6c0cee7ddace1bc0974\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-flbgnvcw/wheels/91/4f/39/609be1ef19e1389c81b6fb0c87fe71eab63c3d1a9e036097ea\n",
      "Successfully built fasttext\n",
      "Installing collected packages: pybind11, fasttext\n",
      "Successfully installed fasttext-0.9.2 pybind11-2.13.3\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/facebookresearch/fastText.git\n",
    "! pip3 install fastText/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 4M words\n",
      "Number of words:  365368\n",
      "Number of labels: 23\n",
      "Progress: 100.0% words/sec/thread:  493399 lr:  0.000000 avg.loss:  0.033720 ETA:   0h 0m 0s 0.115623 avg.loss:  0.043694 ETA:   0h 1m21s\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "\n",
    "ft_model = fasttext.train_supervised(\n",
    "    input='lenta_train.txt',\n",
    "    label='__label__',\n",
    "    lr=0.5,\n",
    "    epoch=75,\n",
    "    wordNgrams=2, \n",
    "    dim=200,\n",
    "    thread=2,\n",
    "    verbose=3000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('__label__2',)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = ft_model.predict('армия')[0]\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = ft_model.predict(X_test)[0]\n",
    "predicted_labels = [int(label[0][9:]) for label in predicted_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.7414746892997878\n",
      "Precision score:  0.7373607694178356\n",
      "Recall score:  0.7414746892997878\n",
      "f1-score:  0.7376333990379118\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.63      0.75        19\n",
      "           1       0.72      0.77      0.75     40517\n",
      "           2       0.73      0.78      0.75     34008\n",
      "           3       0.75      0.78      0.76     19749\n",
      "           4       0.70      0.63      0.66     11207\n",
      "           5       0.94      0.91      0.92     16047\n",
      "           6       0.79      0.79      0.79     13469\n",
      "           7       0.53      0.43      0.47      6938\n",
      "           8       0.55      0.36      0.43      4950\n",
      "           9       0.73      0.74      0.74     13223\n",
      "          10       0.78      0.79      0.78     13230\n",
      "          11       0.75      0.69      0.72      5368\n",
      "          14       0.59      0.49      0.53      1561\n",
      "          15       0.77      0.60      0.67      2014\n",
      "          16       0.67      0.22      0.33        27\n",
      "          17       0.47      0.30      0.37      1867\n",
      "          19       0.00      0.00      0.00         1\n",
      "          20       0.59      0.30      0.40       318\n",
      "          21       0.12      0.05      0.07        65\n",
      "          22       0.54      0.37      0.44       166\n",
      "\n",
      "    accuracy                           0.74    184744\n",
      "   macro avg       0.63      0.53      0.57    184744\n",
      "weighted avg       0.74      0.74      0.74    184744\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "print(\"Accuracy score: \", accuracy_score(y_test, predicted_labels))\n",
    "print(\"Precision score: \", precision_score(y_test, predicted_labels, average='weighted'))\n",
    "print(\"Recall score: \", recall_score(y_test, predicted_labels, average='weighted'))\n",
    "print(\"f1-score: \", f1_score(y_test, predicted_labels, average='weighted'))\n",
    "\n",
    "report = classification_report(y_test, predicted_labels)\n",
    "print(\"\\nClassification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
