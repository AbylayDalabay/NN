{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">2. Simple Token-Based Search Using Lemmatization and Stemming. This task involves creating a simple token-based search using lemmatization and stemming techniques. Below is a Python function template that takes a user's input and returns the most relevant sentence from a set of 50 sentences.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "banking_sentences = []\n",
    "with open('banking_sentences.txt', 'r', encoding='utf-8') as file:\n",
    "    banking_sentences = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punctuation and newlines\n",
    "import string\n",
    "exclude = set(string.punctuation)\n",
    "exclude.add('\\n')\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return ''.join(char for char in text if char not in exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# stopwords removing\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words_en = list(get_stop_words('en'))        \n",
    "nltk_words_en = list(stopwords.words('english')) \n",
    "stop_words_ru = list(get_stop_words('ru'))         \n",
    "nltk_words_ru = list(stopwords.words('russian')) \n",
    "\n",
    "stop_words_en = set(get_stop_words('en')) | set(stopwords.words('english'))\n",
    "stop_words_ru = set(get_stop_words('ru')) | set(stopwords.words('russian'))\n",
    "\n",
    "stop_words_en_ru = set(stop_words_en) | set(stop_words_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    return ' '.join(word for word in text.split() if word.lower() not in stop_words_en_ru)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizers\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "en_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from pymystem3 import Mystem\n",
    "ru_lemmatizer = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemmers\n",
    "from nltk.stem import PorterStemmer\n",
    "en_stemmer = PorterStemmer()\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "ru_stemmer = SnowballStemmer(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language detector\n",
    "from lingua import Language, LanguageDetectorBuilder\n",
    "languages = [Language.RUSSIAN, Language.ENGLISH]\n",
    "lang_detector = LanguageDetectorBuilder.from_languages(*languages).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "def nltk_tokenize(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['кошка', '\\n']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_lemmatizer.lemmatize('кошек')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatized sentences\n",
    "# with open('banking_sentences_lemmatized.txt', 'a', encoding='utf-8') as file:\n",
    "#     file.truncate(0)\n",
    "#     for index, sentence in enumerate(banking_sentences):\n",
    "#         tokenized_sentence = nltk_tokenize(remove_punctuation(remove_stopwords(sentence)))\n",
    "#         root_sentence = []\n",
    "#         for word in tokenized_sentence:\n",
    "#             detected_lang = lang_detector.detect_language_of(word.lower())\n",
    "#             if detected_lang == Language.ENGLISH:\n",
    "#                 root_sentence.append(en_lemmatizer.lemmatize(word.lower()))\n",
    "#             elif detected_lang == Language.RUSSIAN:\n",
    "#                 root_sentence.append(ru_lemmatizer.lemmatize(word.lower())[0])\n",
    "\n",
    "#         print(index, \"/\", len(banking_sentences), \": \", root_sentence)\n",
    "#         file.write(' '.join(root_sentence) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemmed sentences\n",
    "# with open('banking_sentences_stemmed.txt', 'a', encoding='utf-8') as file:\n",
    "#     file.truncate(0)\n",
    "#     for index, sentence in enumerate(banking_sentences):\n",
    "#         tokenized_sentence = nltk_tokenize(remove_punctuation(remove_stopwords(sentence)))\n",
    "#         root_sentence = []\n",
    "#         for word in tokenized_sentence:\n",
    "#             detected_lang = lang_detector.detect_language_of(word.lower())\n",
    "#             if detected_lang == Language.ENGLISH:\n",
    "#                 root_sentence.append(en_stemmer.stem(word.lower()))\n",
    "#             elif detected_lang == Language.RUSSIAN:\n",
    "#                 root_sentence.append(ru_stemmer.stem(word.lower()))\n",
    "\n",
    "#         print(index, \"/\", len(banking_sentences), \": \", root_sentence)\n",
    "#         file.write(' '.join(root_sentence) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence, approach='lemmatize'):\n",
    "    root_sentence = []\n",
    "    if approach == 'lemmatize':\n",
    "        tokenized_sentence = nltk_tokenize(remove_punctuation(remove_stopwords(sentence)))\n",
    "        for word in tokenized_sentence:\n",
    "            detected_lang = lang_detector.detect_language_of(word.lower())\n",
    "            if detected_lang == Language.ENGLISH:\n",
    "                root_sentence.append(en_lemmatizer.lemmatize(word.lower()))\n",
    "            elif detected_lang == Language.RUSSIAN:\n",
    "                root_sentence.append(ru_lemmatizer.lemmatize(word.lower())[0])\n",
    "    elif approach == 'stemming':\n",
    "        tokenized_sentence = nltk_tokenize(remove_punctuation(remove_stopwords(sentence)))\n",
    "        root_sentence = []\n",
    "        for word in tokenized_sentence:\n",
    "            detected_lang = lang_detector.detect_language_of(word.lower())\n",
    "            if detected_lang == Language.ENGLISH:\n",
    "                root_sentence.append(en_stemmer.stem(word.lower()))\n",
    "            elif detected_lang == Language.RUSSIAN:\n",
    "                root_sentence.append(ru_stemmer.stem(word.lower())) \n",
    "    print(root_sentence)         \n",
    "    return root_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "banking_sentences_lemmatized = [line.split() for line in open('banking_sentences_lemmatized.txt', 'r', encoding='utf-8').readlines()]\n",
    "banking_sentences_stemmed = [line.split() for line in open('banking_sentences_stemmed.txt', 'r', encoding='utf-8').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(sentence, approach='lemmatize'):\n",
    "    root_sentence_set = set(preprocess(sentence, approach))\n",
    "    base = banking_sentences_lemmatized if approach == 'lemmatize' else banking_sentences_stemmed\n",
    "\n",
    "    match_scores = [len(root_sentence_set.intersection(base_sentence)) for base_sentence in base]\n",
    "\n",
    "    max_score = max(match_scores)\n",
    "\n",
    "    max_index = match_scores.index(max_score)\n",
    "\n",
    "    print('Approach: ', approach)\n",
    "    print(banking_sentences[max_index])\n",
    "    print(\"Max score: \", max_score)\n",
    "\n",
    "    return max_score, max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['сберегательный', 'счет', 'банка']\n",
      "Approach:  lemmatize\n",
      "Банк предлагает разнообразные сберегательные счета с разными процентными ставками, чтобы удовлетворить индивидуальные потребности.\n",
      "\n",
      "Max score:  2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 51)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'сберегательные счета банка'\n",
    "search(query, 'lemmatize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['сберегательн', 'счет', 'банк']\n",
      "Approach:  stemming\n",
      "Банк предлагает разнообразные сберегательные счета с разными процентными ставками, чтобы удовлетворить индивидуальные потребности.\n",
      "\n",
      "Max score:  3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 51)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(query, 'stemming')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">3. Text Cleaning by Removing Blacklist Words or Phrases</font>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a sample text including the **** ***** is ******* and *** and ****** **** ******\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import bigrams\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "blacklist = ['nigga', 'uzbek', 'гей', 'Токаев лучший', 'Путин', 'Токаев Тигр']\n",
    "\n",
    "def clean_text(text, threshold=0.6):\n",
    "    # tokens = nltk_tokenize(remove_punctuation(remove_stopwords(text.lower())))\n",
    "    # bigram_list = list(bigrams(tokens))\n",
    "\n",
    "    split_text = text.split(' ')\n",
    "    hide_indices = []\n",
    "    for i in range(len(split_text)):\n",
    "        match_scores = [SequenceMatcher(None, split_text[i], bad_word).ratio() for bad_word in blacklist]\n",
    "        if max(match_scores) > threshold and split_text[i] not in stop_words_en_ru:\n",
    "            hide_indices.append((i, i))\n",
    "\n",
    "    for i in range(len(split_text) - 1):\n",
    "        match_scores_bigram = [SequenceMatcher(None, split_text[i] + split_text[i + 1], bad_word).ratio() for bad_word in blacklist]\n",
    "        if max(match_scores_bigram) > threshold and split_text[i] not in stop_words_en_ru and split_text[i + 1] not in stop_words_en_ru:\n",
    "            hide_indices.append((i, i + 1))\n",
    "\n",
    "    for i, j in hide_indices:\n",
    "        split_text[i] = '*' * len(split_text[i])\n",
    "        split_text[j] = '*' * len(split_text[j])\n",
    "\n",
    "    return ' '.join(split_text)\n",
    "\n",
    "# Example usage\n",
    "sample_text = \"Here is a sample text including the word Путин is niggers and гей and Токаев Тигр uzbek.\"\n",
    "print(clean_text(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
