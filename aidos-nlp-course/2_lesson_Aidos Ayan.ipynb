{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59265d42-f171-4eef-8e0a-2a772239be59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['http_proxy'] = \"http://proxy-ws.cbank.kz:8080\"\n",
    "# os.environ['https_proxy'] = \"http://proxy-ws.cbank.kz:8080\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7e8f04-2e37-41d0-a47b-5ed78584bc52",
   "metadata": {},
   "source": [
    "<font size=\"12\">Sentenize</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e41da82-27db-41c7-8f2d-4d745dc9ebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_en = \"Hello! How are you? I hope you're doing well. Have a great day.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d553be7d-4110-481c-8277-a05a1a23dddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ru = '''\n",
    "... - \"–¢–∞–∫ –≤ —á–µ–º –∂–µ –¥–µ–ª–æ?\" - \"–ù–µ —Ä–∞-–¥—É-—é—Ç\".\n",
    "... –ò —Ç. –¥. –∏ —Ç. –ø. –í –æ–±—â–µ–º, –≤—Å—è –≥–∞–∑–µ—Ç–∞\n",
    "... '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61f98a84-e5ff-42a5-9b58-24a6c2b4775f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def nltk_sentenize(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98f0c903-e8fe-477b-90ac-ee44d1b6fdb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello!', 'How are you?', \"I hope you're doing well.\", 'Have a great day.']\n",
      "['\\n- \"–¢–∞–∫ –≤ —á–µ–º –∂–µ –¥–µ–ª–æ?\"', '- \"–ù–µ —Ä–∞-–¥—É-—é—Ç\".', '–ò —Ç. –¥. –∏ —Ç. –ø. –í –æ–±—â–µ–º, –≤—Å—è –≥–∞–∑–µ—Ç–∞']\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk_sentenize(text_en)\n",
    "print(sentences)\n",
    "sentences = nltk_sentenize(text_ru)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1502dacf-7ce2-450e-a60b-c9af51cfa81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: razdel in c:\\users\\ayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install razdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fca9f8d-db44-44f1-a80c-ad6d0b10a64d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Substring(0, 6, 'Hello!'),\n",
       " Substring(7, 19, 'How are you?'),\n",
       " Substring(20, 45, \"I hope you're doing well.\"),\n",
       " Substring(46, 63, 'Have a great day.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from razdel import sentenize\n",
    "sentences =list(sentenize(text_en))\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e29f05e-d99b-4e87-9cb3-d783b7fc2b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Substring(1, 23, '- \"–¢–∞–∫ –≤ —á–µ–º –∂–µ –¥–µ–ª–æ?\"'),\n",
       " Substring(24, 40, '- \"–ù–µ —Ä–∞-–¥—É-—é—Ç\".'),\n",
       " Substring(41, 56, '–ò —Ç. –¥. –∏ —Ç. –ø.'),\n",
       " Substring(57, 76, '–í –æ–±—â–µ–º, –≤—Å—è –≥–∞–∑–µ—Ç–∞')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences =list(sentenize(text_ru))\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbe054d-bfa9-4e84-b570-2c05bfb4f1ab",
   "metadata": {},
   "source": [
    "<font size=\"12\">Tokenizer</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0919e8b9-1b4e-4ad1-9580-7f8c9a3a85ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '–ö—Ä—É–∂–∫–∞-—Ç–µ—Ä–º–æ—Å –Ω–∞ 0.5–ª (50/64 —Å–º¬≥, 516;...)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc938b8f-fcc4-4f7b-aeb0-90075cf8cb9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8905d0c6-4b36-4573-a051-43b36e724266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['–ö—Ä—É–∂–∫–∞-—Ç–µ—Ä–º–æ—Å', '–Ω–∞', '0.5–ª', '(50/64', '—Å–º¬≥,', '516;...)']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = text.split()\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10f5524d-ae12-4d9a-91b5-5d20fee3ab4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['–ö—Ä—É–∂–∫–∞-—Ç–µ—Ä–º–æ—Å',\n",
       " '–Ω–∞',\n",
       " '0.5–ª',\n",
       " '(',\n",
       " '50/64',\n",
       " '—Å–º¬≥',\n",
       " ',',\n",
       " '516',\n",
       " ';',\n",
       " '...',\n",
       " ')']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nltk_tokenize(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return words\n",
    "nltk_tokenize(text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "860ccd95-8179-430e-b491-e47fd8aecd2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Substring(0, 13, '–ö—Ä—É–∂–∫–∞-—Ç–µ—Ä–º–æ—Å'),\n",
       " Substring(14, 16, '–Ω–∞'),\n",
       " Substring(17, 20, '0.5'),\n",
       " Substring(20, 21, '–ª'),\n",
       " Substring(22, 23, '('),\n",
       " Substring(23, 28, '50/64'),\n",
       " Substring(29, 32, '—Å–º¬≥'),\n",
       " Substring(32, 33, ','),\n",
       " Substring(34, 37, '516'),\n",
       " Substring(37, 38, ';'),\n",
       " Substring(38, 41, '...'),\n",
       " Substring(41, 42, ')')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from razdel import tokenize\n",
    "tokens = list(tokenize(text))\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65da3b9b-57a3-445b-a3c3-79272f355508",
   "metadata": {},
   "source": [
    "<font size=\"12\">Convert  emoticons to text</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0c689ec-d497-449f-93b2-b20e1983af79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             text\n",
      "0  Hello Happy How are you? laugh\n",
      "1            I'm feeling down Sad\n",
      "2         That's hilarious! laugh\n",
      "3        I can't believe it laugh\n",
      "4            This is so sad laugh\n",
      "5            Wink and smile Happy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    \"text\": [\n",
    "        \"Hello :) How are you? :D\",\n",
    "        \"I'm feeling down :(\",\n",
    "        \"That's hilarious! :D\",\n",
    "        \"I can't believe it laugh-)\",\n",
    "        \"This is so sad laugh-(\",\n",
    "        \";) and smile :)\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define the replacements\n",
    "replacements = [\n",
    "    (r\"\\:\\)\", \"Happy\"),\n",
    "    (r\"\\:\\-\\)\", \"Happy\"),\n",
    "    (r\"\\:\\-\\}\", \"Happy\"),\n",
    "    (r\"\\;\\-\\}\", \"Happy\"),\n",
    "    (r\"\\:\\-\\>\", \"Happy\"),\n",
    "    (r\"\\;\\-\\)\", \"Happy\"),\n",
    "    (r\"\\;\\)\", \"Wink\"),\n",
    "    (r\"\\:\\-\\(\", \"Sad\"),\n",
    "    (r\"\\:\\(\", \"Sad\"),\n",
    "    (r\"\\:\\-\\|\", \"Sad\"),\n",
    "    (r\"\\;\\-\\(\", \"Sad\"),\n",
    "    (r\"\\;\\-\\<\", \"Sad\"),\n",
    "    (r\"\\|\\-\\{\", \"Sad\"),\n",
    "    (r\"\\:\\D\", \"laugh\"),\n",
    "    (r\"\\:\\'\\-\\)\", \"tear of joy\"),\n",
    "    (r\"\\:\\`\\-\\(\", \"tear of sadness\"),\n",
    "    (r\"laugh-\\)\", \"laugh\"),\n",
    "    (r\"laugh-\\(\", \"laugh\")\n",
    "]\n",
    "\n",
    "# Apply the replacements\n",
    "for pattern, replacement in replacements:\n",
    "    df[\"text\"] = df[\"text\"].str.replace(pattern, replacement, regex=True)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aead00d-b9db-4e82-9cc3-4a143bb576b6",
   "metadata": {},
   "source": [
    "\n",
    "<font size=\"12\">Removing punctuation </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f35c7eb7-12da-4ba2-a1d6-f594367ab9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world! Hows  everything going\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Define the set of punctuation to exclude\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "# Define the set of punctuation to retain\n",
    "not_exclude = {\".\", \"!\", \",\"}\n",
    "\n",
    "# Determine the final set of punctuation to remove\n",
    "final = exclude - not_exclude\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return ''.join(char for char in text if char not in final)\n",
    "\n",
    "# Example usage\n",
    "text = \"Hello, world! How's ()& everything going?\"\n",
    "cleaned_text = remove_punctuation(text)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addc5e49-783f-44db-9afb-bce3638ad86a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7074581d-4811-4263-b1f6-cdc5ace0075c",
   "metadata": {},
   "source": [
    "<font size=\"12\">Removing stop words</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "870e0d14-85ee-4fa6-9539-31d0529e4c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebee7bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stop_words in c:\\users\\ayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2018.7.23)\n"
     ]
    }
   ],
   "source": [
    "!pip install stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d221671-9812-434f-8e54-1b0b7c5a5b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words_en = list(get_stop_words('en'))        \n",
    "nltk_words_en = list(stopwords.words('english')) \n",
    "stop_words_ru = list(get_stop_words('ru'))         \n",
    "nltk_words_ru = list(stopwords.words('russian')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "297861d4-984e-4cb0-9033-2d6ad7d3de7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_words_en = set(get_stop_words('en')) | set(stopwords.words('english'))\n",
    "#stop_words_ru = set(get_stop_words('ru')) | set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c2ebf05-8eb4-44f9-ad74-6258cb6f911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_en=stop_words_en\n",
    "stop_words_ru=stop_words_ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "526b3bea-16cc-45c4-9a9f-4e3574d0dc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example sentence demonstrate removal stopwords.\n",
      "–ø—Ä–∏–º–µ—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —É–¥–∞–ª–µ–Ω–∏—è —Å—Ç–æ–ø-—Å–ª–æ–≤.\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(text, language='en'):\n",
    "    if language == 'en':\n",
    "        stop_words = stop_words_en\n",
    "    elif language == 'ru':\n",
    "        stop_words = stop_words_ru\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported language. Use 'en' for English or 'ru' for Russian.\")\n",
    "    \n",
    "    return ' '.join(word for word in text.split() if word.lower() not in stop_words)\n",
    "\n",
    "# Example usage\n",
    "english_text = \"This is an example sentence to demonstrate the removal of stopwords.\"\n",
    "russian_text = \"–≠—Ç–æ –ø—Ä–∏–º–µ—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —É–¥–∞–ª–µ–Ω–∏—è —Å—Ç–æ–ø-—Å–ª–æ–≤.\"\n",
    "\n",
    "cleaned_english_text = remove_stopwords(english_text, 'en')\n",
    "print(cleaned_english_text)\n",
    "cleaned_russian_text = remove_stopwords(russian_text, 'ru')\n",
    "print(cleaned_russian_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dba7fc-ded6-49af-aae2-45d96ab538b7",
   "metadata": {},
   "source": [
    "<font size=\"12\">Lemmatizer and Stemmer</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed619680-6d12-438e-b788-d4a88a61a977",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Ayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e227f2f-89b3-469f-a329-19176fad7b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kites ---> kite\n",
      "babies ---> baby\n",
      "dogs ---> dog\n",
      "flying ---> flying\n",
      "smiling ---> smiling\n",
      "driving ---> driving\n",
      "died ---> died\n",
      "tried ---> tried\n",
      "feet ---> foot\n",
      "running ---> running\n",
      "are ---> are\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "# Create WordNetLemmatizer object\n",
    "wnl = WordNetLemmatizer()\n",
    " \n",
    "# single word lemmatization examples\n",
    "list1 = ['kites', 'babies', 'dogs', 'flying', 'smiling', \n",
    "         'driving', 'died', 'tried', 'feet', 'running', 'are']\n",
    "for words in list1:\n",
    "    print(words + \" ---> \" + wnl.lemmatize(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eaa98a48-de41-4f41-a2fe-446c9f06884a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"running\")\n",
    "doc[0].lemma_\n",
    "# for token in doc:\n",
    "#     print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7d43021-cbaa-48ae-99c1-6b0ddc8e1a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmer.stem(\"running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a22c97dc-ea63-4a91-9506-98e08bbbd784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: running | Stem: run | Lemma: run\n",
      "Word: runs | Stem: run | Lemma: run\n",
      "Word: runner | Stem: runner | Lemma: runner\n",
      "Word: easily | Stem: easili | Lemma: easily\n",
      "Word: fairly | Stem: fairli | Lemma: fairly\n",
      "Word: cats | Stem: cat | Lemma: cat\n",
      "Word: studies | Stem: studi | Lemma: study\n",
      "Word: studying | Stem: studi | Lemma: study\n",
      "Word: better | Stem: better | Lemma: better\n",
      "Word: was | Stem: wa | Lemma: be\n",
      "Word: are | Stem: are | Lemma: be\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a list of words to demonstrate the difference\n",
    "words = [\"running\", \"runs\", \"runner\", \"easily\", \"fairly\", \"cats\", \"studies\", \"studying\", \"better\", \"was\", \"are\"]\n",
    "\n",
    "# Apply stemming and lemmatization\n",
    "stems = [stemmer.stem(word) for word in words]\n",
    "lemmas = [lemmatizer.lemmatize(word, pos='v') for word in words]  # Using 'v' for verb to get accurate results for verbs\n",
    "\n",
    "# Display the results\n",
    "for word, stem, lemma in zip(words, stems, lemmas):\n",
    "    print(f\"Word: {word} | Stem: {stem} | Lemma: {lemma}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c77a82d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymystem3 in c:\\users\\ayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\ayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pymystem3) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->pymystem3) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->pymystem3) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->pymystem3) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->pymystem3) (2023.5.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07739b3b-f043-41f9-8013-569cf551edca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: –ö—Ä–∞—Å–∏–≤–∞—è | Lemma: –∫—Ä–∞—Å–∏–≤—ã–π | Stem: –∫—Ä–∞—Å–∏–≤\n",
      "Word: –±–µ–≥–∞—é—â–∞—è | Lemma: –±–µ–≥–∞—Ç—å | Stem: –±–µ–≥–∞\n",
      "Word: –º–∞–º–∞ | Lemma: –º–∞–º–∞ | Stem: –º–∞–º\n",
      "Word: –∫—Ä–∞—Å–∏–≤–æ | Lemma: –∫—Ä–∞—Å–∏–≤–æ | Stem: –∫—Ä–∞—Å–∏–≤\n",
      "Word: –º—ã–ª–∞ | Lemma: –º—ã—Ç—å | Stem: –º—ã–ª\n",
      "Word: —Ä–∞–º—É | Lemma: —Ä–∞–º–∞ | Stem: —Ä–∞–º\n",
      "Word: —É–∑–Ω–∞–≤ | Lemma: —É–∑–Ω–∞–≤–∞—Ç—å | Stem: —É–∑–Ω–∞\n",
      "Word: –ø—Ä–∞–≤–¥—É | Lemma: –ø—Ä–∞–≤–¥–∞ | Stem: –ø—Ä–∞–≤–¥\n"
     ]
    }
   ],
   "source": [
    "from pymystem3 import Mystem\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk\n",
    "\n",
    "# Initialize the Mystem and SnowballStemmer\n",
    "mystem = Mystem()\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "\n",
    "# Input text\n",
    "text = \"–ö—Ä–∞—Å–∏–≤–∞—è –±–µ–≥–∞—é—â–∞—è –º–∞–º–∞ –∫—Ä–∞—Å–∏–≤–æ –º—ã–ª–∞ —Ä–∞–º—É —É–∑–Ω–∞–≤ –ø—Ä–∞–≤–¥—É\"\n",
    "\n",
    "# Perform lemmatization\n",
    "lemmas = mystem.lemmatize(text)\n",
    "lemmas = [lemma.strip() for lemma in lemmas if lemma.strip()]\n",
    "\n",
    "# Perform stemming\n",
    "words = text.split()\n",
    "stems = [stemmer.stem(word) for word in words]\n",
    "\n",
    "# Display the results\n",
    "for word, lemma, stem in zip(words, lemmas, stems):\n",
    "    print(f\"Word: {word} | Lemma: {lemma} | Stem: {stem}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fd0e8b-81a4-4520-9d4b-c331281365bc",
   "metadata": {},
   "source": [
    "\n",
    "<font size=\"12\">Home tasks</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf65f8c-a6be-40d6-9593-6c55ec2852f1",
   "metadata": {},
   "source": [
    "<font size=\"5\">1. This task involves normalizing and cleaning a text where some words are replaced with emoticons or the text is split in an unusual way </font>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "8fb83b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "e4b13bc8-09e7-4a36-94bc-3188c7b14c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "–í—á–µ—Ä–∞-–±—ã–ª –ø—Ä–æ—Å—Ç–æ —Å—É–ø–µ—Ä üòä! –ü—Ä–æ–µ—Ö–∞–ª 100/160 –∫–º, —á—Ç–æ–±—ã –≤—Å—Ç—Ä–µ-—Ç–∏—Ç—å—Å—è —Å–æ —Å–≤–æ–∏–º –¥—Ä—É–≥–æ–º. –≠–≥–æ –∏–º—è–ê—Å—Ö–∞—Ç –ê.–ù. –µ–º—É24–≥.–≠—Ç–æ –±—ã–ª–æ –Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ –∫—Ä—É—Ç–æ!–û–±—Å—É–∂–¥–∞–ª–∏ –≤—Å–µ, –æ—Ç –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è-–¥–æ –ø–æ–ª–∏—Ç–∏–∫–∏, 2-3—á–∞—Å–∞ –∏–ª–∏–≤–µ—á–Ω–æ—Å—Ç—å –∑–∞ –æ–±–µ–¥–æ–º –≤ —Å—É—à–∏-–±–∞—Ä–µ, –•–ê–•–ê üòÇ. –ü–æ—Ç–æ–º –ø–æ—Å–º–æ—Ç—Ä–µ–ª–∏ —Ñ–∏–ª—å–º–§–û—Ä–µ—Å—Ç –ì.–≤ –∫–∏–Ω–æ –∏ –∑–∞–≥–æ–≤–æ—Ä–∏–ª–∏ –æ —Ç—Ä–µ–Ω–¥–∞—Ö –Ω–∞ —Ä—ã–Ω–∫–µ –∏ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏—Ö –≤–ª–∏—è–Ω–∏—è—Ö.–ü–æ–≥–æ–¥–∞? –ù–∞—á–∞–ª–æ—Å—å –≤—Å–µ —Å  üåû, –Ω–æ –≤–¥—Ä—É–≥ –ø–æ—à–µ–ª  üåßÔ∏è. –ü–µ—Ä–µ–¥ –æ—Ç—ä–µ–∑–¥–æ–º –∑–∞—à–µ–ª –≤ —É—é—Ç–Ω–æ–µ –∫–∞—Ñ–µ –∑–∞ —á–∞—à–∫–æ–π ‚òï, –æ—á–µ–Ω—å —É—é—Ç–Ω–æ.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "88f84e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "06e4a891",
   "metadata": {},
   "outputs": [],
   "source": [
    "russian_spellchecker = SpellChecker(language='ru', distance = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "dc10e76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(text):\n",
    "    for i in range(len(text)):\n",
    "        if i > 0 and i < (len(text) - 1):\n",
    "            if text[i - 1].isalpha() and text[i + 1].isalpha() and not text[i].isalpha():\n",
    "                text = text.replace(text[i], ' ') \n",
    "                corrected = russian_spellchecker.correction(text)\n",
    "                if corrected: \n",
    "                    text = corrected\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "6fceefd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func2(text):\n",
    "    for i in range(len(text)):\n",
    "        if i > 0 and i < (len(text) - 1):\n",
    "            if text[i - 1].islower() and text[i].isupper():\n",
    "                text = text.replace(text[i], ' ' + text[i]) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "e5134b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'–í—á–µ—Ä–∞ –±—ã–ª –ø—Ä–æ—Å—Ç–æ —Å—É–ø–µ—Ä üòä ! –ü—Ä–æ–µ—Ö–∞–ª 100/160 –∫–º , —á—Ç–æ–±—ã –≤—Å—Ç—Ä–µ—Ç–∏—Ç—å—Å—è —Å–æ —Å–≤–æ–∏–º –¥—Ä—É–≥–æ–º . –≠–≥–æ –∏–º—è –ê—Å—Ö–∞—Ç –ê . –ù . –µ–º—É 24 –≥ . –≠—Ç–æ –±—ã–ª–æ –Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ –∫—Ä—É—Ç–æ ! –û–±—Å—É–∂–¥–∞–ª–∏ –≤—Å–µ , –æ—Ç –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –¥–æ –ø–æ–ª–∏—Ç–∏–∫–∏ , 2-3 —á–∞—Å–∞ –∏–ª–∏–≤–µ—á–Ω–æ—Å—Ç—å –∑–∞ –æ–±–µ–¥–æ–º –≤ —Å—É—à–∏ –±–∞—Ä–µ , –•–ê–•–ê üòÇ . –ü–æ—Ç–æ–º –ø–æ—Å–º–æ—Ç—Ä–µ–ª–∏ —Ñ–∏–ª—å–º –§–û—Ä–µ—Å—Ç –ì . –≤ –∫–∏–Ω–æ –∏ –∑–∞–≥–æ–≤–æ—Ä–∏–ª–∏ –æ —Ç—Ä–µ–Ω–¥–∞—Ö –Ω–∞ —Ä—ã–Ω–∫–µ –∏ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏—Ö –≤–ª–∏—è–Ω–∏—è—Ö . –ü–æ–≥–æ–¥–∞ ? –ù–∞—á–∞–ª–æ—Å—å –≤—Å–µ —Å üåû , –Ω–æ –≤–¥—Ä—É–≥ –ø–æ—à–µ–ª üåßÔ∏è . –ü–µ—Ä–µ–¥ –æ—Ç—ä–µ–∑–¥–æ–º –∑–∞—à–µ–ª –≤ —É—é—Ç–Ω–æ–µ –∫–∞—Ñ–µ –∑–∞ —á–∞—à–∫–æ–π ‚òï , –æ—á–µ–Ω—å —É—é—Ç–Ω–æ . '"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text = ''\n",
    "for i in sentenize(text):\n",
    "    for j in tokenize(i.text):\n",
    "        new_text += func2(func(j.text)) + ' '\n",
    "new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "2c68b22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "   text_normalized = re.sub(r'\\s*\\.\\s*', '.', text)\n",
    "   text_normalized = re.sub(r'\\s*\\!\\s*', '! ', text_normalized)\n",
    "   text_normalized = re.sub(r'\\s*\\,\\s*', ', ', text_normalized)\n",
    "   text = text_normalized\n",
    "   for i in range(len(text)):\n",
    "      if i > 0 and i < (len(text) - 1):\n",
    "         if text[i - 1].isupper() and text[i + 1].islower() and text[i] == '.':\n",
    "            text = text.replace(text[i], text[i] + ' ')\n",
    "   for i in range(len(text)):\n",
    "      if i > 0 and i < (len(text) - 1):\n",
    "         if text[i - 1].isupper() and text[i + 1].islower() and text[i].isupper():\n",
    "            text = text.replace(text[i], text[i].lower())\n",
    " # Capitalize the first letter\n",
    "   return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "89fc396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unions = ['–∏–ª–∏', '–Ω–æ']\n",
    "\n",
    "def divide_unions(text):\n",
    "   for union in unions:\n",
    "      if text.startswith(union) or text.endswith(union):\n",
    "         text = text.replace(union, ' ' + union + ' ').strip()\n",
    "         corrected = russian_spellchecker.correction(text)\n",
    "         if corrected: \n",
    "            text = corrected\n",
    "   return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "06847e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'–í—á–µ—Ä–∞ –±—ã–ª –ø—Ä–æ—Å—Ç–æ —Å—É–ø–µ—Ä üòä ! –ü—Ä–æ–µ—Ö–∞–ª 100/160 –∫–º , —á—Ç–æ–±—ã –≤—Å—Ç—Ä–µ—Ç–∏—Ç—å—Å—è —Å–æ —Å–≤–æ–∏–º –¥—Ä—É–≥–æ–º . –≠–≥–æ –∏–º—è –ê—Å—Ö–∞—Ç –ê . –ù . –µ–º—É 24 –≥ . –≠—Ç–æ –±—ã–ª–æ –Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ –∫—Ä—É—Ç–æ ! –û–±—Å—É–∂–¥–∞–ª–∏ –≤—Å–µ , –æ—Ç –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –¥–æ –ø–æ–ª–∏—Ç–∏–∫–∏ , 2-3 —á–∞—Å–∞ –∏–ª–∏–≤–µ—á–Ω–æ—Å—Ç—å –∑–∞ –æ–±–µ–¥–æ–º –≤ —Å—É—à–∏ –±–∞—Ä–µ , –•–ê–•–ê üòÇ . –ü–æ—Ç–æ–º –ø–æ—Å–º–æ—Ç—Ä–µ–ª–∏ —Ñ–∏–ª—å–º –§–û—Ä–µ—Å—Ç –ì . –≤ –∫–∏–Ω–æ –∏ –∑–∞–≥–æ–≤–æ—Ä–∏–ª–∏ –æ —Ç—Ä–µ–Ω–¥–∞—Ö –Ω–∞ —Ä—ã–Ω–∫–µ –∏ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏—Ö –≤–ª–∏—è–Ω–∏—è—Ö . –ü–æ–≥–æ–¥–∞ ? –ù–∞—á–∞–ª–æ—Å—å –≤—Å–µ —Å üåû , –Ω–æ –≤–¥—Ä—É–≥ –ø–æ—à–µ–ª üåßÔ∏è . –ü–µ—Ä–µ–¥ –æ—Ç—ä–µ–∑–¥–æ–º –∑–∞—à–µ–ª –≤ —É—é—Ç–Ω–æ–µ –∫–∞—Ñ–µ –∑–∞ —á–∞—à–∫–æ–π ‚òï , –æ—á–µ–Ω—å —É—é—Ç–Ω–æ . '"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "8e575055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'–í—á–µ—Ä–∞ –±—ã–ª –ø—Ä–æ—Å—Ç–æ —Å—É–ø–µ—Ä üòä ! –ü—Ä–æ–µ—Ö–∞–ª 100/160 –∫–º , —á—Ç–æ–±—ã –≤—Å—Ç—Ä–µ—Ç–∏—Ç—å—Å—è —Å–æ —Å–≤–æ–∏–º –¥—Ä—É–≥–æ–º . –≠–≥–æ –∏–º—è –ê—Å—Ö–∞—Ç –ê . –ù . –µ–º—É 24 –≥ . –≠—Ç–æ –±—ã–ª–æ –Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ –∫—Ä—É—Ç–æ ! –û–±—Å—É–∂–¥–∞–ª–∏ –≤—Å–µ , –æ—Ç –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –¥–æ –ø–æ–ª–∏—Ç–∏–∫–∏ , 2-3 —á–∞—Å–∞ –∏–ª–∏ –≤–µ—á–Ω–æ—Å—Ç—å –∑–∞ –æ–±–µ–¥–æ–º –≤ —Å—É—à–∏ –±–∞—Ä–µ , –•–ê–•–ê üòÇ . –ü–æ—Ç–æ–º –ø–æ—Å–º–æ—Ç—Ä–µ–ª–∏ —Ñ–∏–ª—å–º –§–û—Ä–µ—Å—Ç –ì . –≤ –∫–∏–Ω–æ –∏ –∑–∞–≥–æ–≤–æ—Ä –∏–ª–∏ –æ —Ç—Ä–µ–Ω–¥–∞—Ö –Ω–∞ —Ä—ã–Ω–∫–µ –∏ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏—Ö –≤–ª–∏—è–Ω–∏—è—Ö . –ü–æ–≥–æ–¥–∞ ? –ù–∞—á–∞–ª–æ—Å—å –≤—Å–µ —Å üåû , –Ω–æ –≤–¥—Ä—É–≥ –ø–æ—à–µ–ª üåßÔ∏è . –ü–µ—Ä–µ–¥ –æ—Ç—ä–µ–∑–¥–æ–º –∑–∞—à–µ–ª –≤ —É—é—Ç–Ω–æ–µ –∫–∞—Ñ–µ –∑–∞ —á–∞—à–∫–æ–π ‚òï , –æ—á–µ–Ω—å —É—é—Ç–Ω–æ . '"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_new_text = ''\n",
    "for i in sentenize(new_text):\n",
    "    for j in tokenize(i.text):\n",
    "        new_new_text += divide_unions(j.text) + ' '\n",
    "new_new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "d0b1c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis = {\n",
    "    'üòä': '',\n",
    "    'üòÇ': '–í–•–•–´–í–•–´–•–í–•–´–ê–•–í–ê–í–ê–´',\n",
    "    'üåû': '—Å–æ–ª–Ω—ã—à–∫–æ',\n",
    "    'üåßÔ∏è': '–¥–æ–∂–¥—å',\n",
    "    '‚òï':'–∫–æ—Ñ–µ'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "79d77e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'–í—á–µ—Ä–∞ –±—ã–ª –ø—Ä–æ—Å—Ç–æ —Å—É–ø–µ—Ä  ! –ü—Ä–æ–µ—Ö–∞–ª 100/160 –∫–º , —á—Ç–æ–±—ã –≤—Å—Ç—Ä–µ—Ç–∏—Ç—å—Å—è —Å–æ —Å–≤–æ–∏–º –¥—Ä—É–≥–æ–º . –≠–≥–æ –∏–º—è –ê—Å—Ö–∞—Ç –ê . –ù . –µ–º—É 24 –≥ . –≠—Ç–æ –±—ã–ª–æ –Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ –∫—Ä—É—Ç–æ ! –û–±—Å—É–∂–¥–∞–ª–∏ –≤—Å–µ , –æ—Ç –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –¥–æ –ø–æ–ª–∏—Ç–∏–∫–∏ , 2-3 —á–∞—Å–∞ –∏–ª–∏ –≤–µ—á–Ω–æ—Å—Ç—å –∑–∞ –æ–±–µ–¥–æ–º –≤ —Å—É—à–∏ –±–∞—Ä–µ , –•–ê–•–ê –í–•–•–´–í–•–´–•–í–•–´–ê–•–í–ê–í–ê–´ . –ü–æ—Ç–æ–º –ø–æ—Å–º–æ—Ç—Ä–µ–ª–∏ —Ñ–∏–ª—å–º –§–û—Ä–µ—Å—Ç –ì . –≤ –∫–∏–Ω–æ –∏ –∑–∞–≥–æ–≤–æ—Ä –∏–ª–∏ –æ —Ç—Ä–µ–Ω–¥–∞—Ö –Ω–∞ —Ä—ã–Ω–∫–µ –∏ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏—Ö –≤–ª–∏—è–Ω–∏—è—Ö . –ü–æ–≥–æ–¥–∞ ? –ù–∞—á–∞–ª–æ—Å—å –≤—Å–µ —Å —Å–æ–ª–Ω—ã—à–∫–æ , –Ω–æ –≤–¥—Ä—É–≥ –ø–æ—à–µ–ª –¥–æ–∂–¥—å . –ü–µ—Ä–µ–¥ –æ—Ç—ä–µ–∑–¥–æ–º –∑–∞—à–µ–ª –≤ —É—é—Ç–Ω–æ–µ –∫–∞—Ñ–µ –∑–∞ —á–∞—à–∫–æ–π –∫–æ—Ñ–µ , –æ—á–µ–Ω—å —É—é—Ç–Ω–æ . '"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_new_new_text = ''\n",
    "for i in sentenize(new_new_text):\n",
    "    for j in tokenize(i.text):\n",
    "        word = j.text\n",
    "        if word in emojis:\n",
    "            word = emojis[word]\n",
    "        new_new_new_text += word + ' '\n",
    "new_new_new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "e27e5abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–í—á–µ—Ä–∞ –±—ã–ª –ø—Ä–æ—Å—Ç–æ —Å—É–ø–µ—Ä üòä! –ü—Ä–æ–µ—Ö–∞–ª 100/160 –∫–º, —á—Ç–æ–±—ã –≤—Å—Ç—Ä–µ—Ç–∏—Ç—å—Å—è —Å–æ —Å–≤–æ–∏–º –¥—Ä—É–≥–æ–º. –≠–≥–æ –∏–º—è –ê—Å—Ö–∞—Ç –ê. –ù. –µ–º—É 24 –≥. –≠—Ç–æ –±—ã–ª–æ –Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ –∫—Ä—É—Ç–æ! –æ–±—Å—É–∂–¥–∞–ª–∏ –≤—Å–µ, –æ—Ç –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –¥–æ –ø–æ–ª–∏—Ç–∏–∫–∏, 2-3 —á–∞—Å–∞ –∏–ª–∏ –≤–µ—á–Ω–æ—Å—Ç—å –∑–∞ –æ–±–µ–¥–æ–º –≤ —Å—É—à–∏ –±–∞—Ä–µ, –•–ê–•–ê üòÇ. –ü–æ—Ç–æ–º –ø–æ—Å–º–æ—Ç—Ä–µ–ª–∏ —Ñ–∏–ª—å–º –§–æ—Ä–µ—Å—Ç –ì. –≤ –∫–∏–Ω–æ –∏ –∑–∞–≥–æ–≤–æ—Ä –∏–ª–∏ –æ —Ç—Ä–µ–Ω–¥–∞—Ö –Ω–∞ —Ä—ã–Ω–∫–µ –∏ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏—Ö –≤–ª–∏—è–Ω–∏—è—Ö. –ü–æ–≥–æ–¥–∞ ? –ù–∞—á–∞–ª–æ—Å—å –≤—Å–µ —Å üåû, –Ω–æ –≤–¥—Ä—É–≥ –ø–æ—à–µ–ª üåßÔ∏è. –ü–µ—Ä–µ–¥ –æ—Ç—ä–µ–∑–¥–æ–º –∑–∞—à–µ–ª –≤ —É—é—Ç–Ω–æ–µ –∫–∞—Ñ–µ –∑–∞ —á–∞—à–∫–æ–π ‚òï, –æ—á–µ–Ω—å —É—é—Ç–Ω–æ. \n"
     ]
    }
   ],
   "source": [
    "final_text = normalize_text(new_new_text)\n",
    "print(final_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642556fb-7a2e-4e09-917d-b9d3ff2df455",
   "metadata": {},
   "source": [
    "<font size=\"5\">2. Simple Token-Based Search Using Lemmatization and Stemming. This task involves creating a simple token-based search using lemmatization and stemming techniques. Below is a Python function template that takes a user's input and returns the most relevant sentence from a set of 50 sentences.</font>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "5cb5bbff-daa7-450f-b54b-1ba558e346ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "banking_sentences = [\n",
    "    \"The bank offers a variety of savings accounts with different interest rates to suit individual needs.\",\n",
    "    \"Customers can easily transfer money between their accounts using the online banking platform.\",\n",
    "    \"Every month, the bank sends out statements detailing all transactions made during the billing cycle.\",\n",
    "    \"Opening a new account at the bank requires a valid ID and proof of address.\",\n",
    "    \"The bank's mobile app allows users to deposit checks by simply taking a photo with their phone.\",\n",
    "    \"Loans for purchasing homes are available with fixed or variable interest rates.\",\n",
    "    \"Credit cards from the bank come with various rewards programs, including cashback and travel points.\",\n",
    "    \"Bank employees are available to help customers understand their financial statements and plan their budgets.\",\n",
    "    \"Savings accounts can be linked to checking accounts to prevent overdraft fees.\",\n",
    "    \"The bank offers financial advice services to help customers plan for retirement.\",\n",
    "    \"Automatic bill payment services help customers avoid missing due dates for important bills.\",\n",
    "    \"Bank branches provide secure safety deposit boxes for storing valuable items.\",\n",
    "    \"Customers can apply for personal loans to cover unexpected expenses or consolidate debt.\",\n",
    "    \"Online banking allows users to set up alerts for low balances or large transactions.\",\n",
    "    \"The bank provides educational resources to help customers understand how to manage their money.\",\n",
    "    \"Customers can choose from a range of investment options, including stocks and bonds.\",\n",
    "    \"The bank has a dedicated customer service line to assist with any account-related issues.\",\n",
    "    \"Mortgage specialists are available to help first-time homebuyers navigate the process.\",\n",
    "    \"The bank's credit monitoring service alerts customers to any changes in their credit reports.\",\n",
    "    \"Customers can easily update their contact information through the bank's online portal.\",\n",
    "    \"The bank's ATM network provides convenient access to cash withdrawals and deposits.\",\n",
    "    \"Foreign currency exchange services are available for customers planning international travel.\",\n",
    "    \"The bank's fraud protection service monitors accounts for suspicious activity.\",\n",
    "    \"Customers can set spending limits on their credit cards to help manage their budgets.\",\n",
    "    \"The bank offers a range of insurance products, including health, auto, and home insurance.\",\n",
    "    \"Users can download and print monthly account statements directly from the bank's website.\",\n",
    "    \"Small business owners can access loans and lines of credit to help grow their businesses.\",\n",
    "    \"The bank provides secure online payment options for shopping on various e-commerce platforms.\",\n",
    "    \"Mobile banking apps allow users to check their account balances on the go.\",\n",
    "    \"The bank's retirement accounts offer tax advantages to help customers save for the future.\",\n",
    "    \"Financial advisors at the bank can help customers develop long-term investment strategies.\",\n",
    "    \"The bank offers low-interest loans for education and other major life expenses.\",\n",
    "    \"Customers can make international money transfers at competitive exchange rates.\",\n",
    "    \"The bank's secure online platform protects customers' personal and financial information.\",\n",
    "    \"Home equity lines of credit are available for homeowners needing access to cash.\",\n",
    "    \"The bank offers fixed-term deposits with higher interest rates for long-term savings.\",\n",
    "    \"Customers can access their account information 24/7 through the bank's mobile app.\",\n",
    "    \"The bank's customer loyalty programs offer benefits such as reduced fees and higher interest rates.\",\n",
    "    \"Parents can open savings accounts for their children to teach them about money management.\",\n",
    "    \"The bank's budgeting tools help customers track their spending and set financial goals.\",\n",
    "    \"Customers can sign up for direct deposit to have their paychecks automatically deposited.\",\n",
    "    \"The bank offers special accounts for students with no monthly fees and low minimum balances.\",\n",
    "    \"Users can schedule future payments and transfers using the bank's online banking services.\",\n",
    "    \"The bank's investment products include mutual funds and retirement accounts.\",\n",
    "    \"Customers can access financial planning services to help manage their assets and liabilities.\",\n",
    "    \"The bank provides loan calculators to help customers estimate their monthly payments.\",\n",
    "    \"Secure messaging through the bank's online platform allows customers to communicate with their advisors.\",\n",
    "    \"The bank's mobile check deposit feature makes it easy to deposit checks without visiting a branch.\",\n",
    "    \"Customers can earn rewards points for every dollar spent on their credit cards.\",\n",
    "    \"The bank offers personal finance workshops to educate customers on budgeting and saving.\",\n",
    "    \"Customers can track their spending habits using the bank's online expense tracking tools.\",\n",
    "    \"–ë–∞–Ω–∫ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —Å–±–µ—Ä–µ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Å—á–µ—Ç–∞ —Å —Ä–∞–∑–Ω—ã–º–∏ –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–º–∏ —Å—Ç–∞–≤–∫–∞–º–∏, —á—Ç–æ–±—ã —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç—å –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–∏.\",\n",
    "    \"–ö–ª–∏–µ–Ω—Ç—ã –º–æ–≥—É—Ç –ª–µ–≥–∫–æ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å –¥–µ–Ω—å–≥–∏ –º–µ–∂–¥—É —Å–≤–æ–∏–º–∏ —Å—á–µ—Ç–∞–º–∏ —Å –ø–æ–º–æ—â—å—é –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–±–∞–Ω–∫–∏–Ω–≥–∞.\",\n",
    "    \"–ö–∞–∂–¥—ã–π –º–µ—Å—è—Ü –±–∞–Ω–∫ –≤—ã—Å—ã–ª–∞–µ—Ç –≤—ã–ø–∏—Å–∫–∏ —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º –æ–ø–∏—Å–∞–Ω–∏–µ–º –≤—Å–µ—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –∑–∞ –æ—Ç—á–µ—Ç–Ω—ã–π –ø–µ—Ä–∏–æ–¥.\",\n",
    "    \"–û—Ç–∫—Ä—ã—Ç–∏–µ –Ω–æ–≤–æ–≥–æ —Å—á–µ—Ç–∞ –≤ –±–∞–Ω–∫–µ —Ç—Ä–µ–±—É–µ—Ç –¥–µ–π—Å—Ç–≤—É—é—â–µ–≥–æ —É–¥–æ—Å—Ç–æ–≤–µ—Ä–µ–Ω–∏—è –ª–∏—á–Ω–æ—Å—Ç–∏ –∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –∞–¥—Ä–µ—Å–∞.\",\n",
    "    \"–ú–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –±–∞–Ω–∫–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –≤–Ω–æ—Å–∏—Ç—å —á–µ–∫–∏, –ø—Ä–æ—Å—Ç–æ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—Ä—É—è –∏—Ö –Ω–∞ —Ç–µ–ª–µ—Ñ–æ–Ω.\",\n",
    "    \"–ö—Ä–µ–¥–∏—Ç—ã –Ω–∞ –ø–æ–∫—É–ø–∫—É –¥–æ–º–æ–≤ –¥–æ—Å—Ç—É–ø–Ω—ã —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∏–ª–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–º–∏ —Å—Ç–∞–≤–∫–∞–º–∏.\",\n",
    "    \"–ö—Ä–µ–¥–∏—Ç–Ω—ã–µ –∫–∞—Ä—Ç—ã –±–∞–Ω–∫–∞ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π, –≤–∫–ª—é—á–∞—è –∫—ç—à–±—ç–∫ –∏ –±–æ–Ω—É—Å–Ω—ã–µ –±–∞–ª–ª—ã –¥–ª—è –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–π.\",\n",
    "    \"–°–æ—Ç—Ä—É–¥–Ω–∏–∫–∏ –±–∞–Ω–∫–∞ –≥–æ—Ç–æ–≤—ã –ø–æ–º–æ—á—å –∫–ª–∏–µ–Ω—Ç–∞–º –ø–æ–Ω—è—Ç—å —Å–≤–æ–∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –≤—ã–ø–∏—Å–∫–∏ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –±—é–¥–∂–µ—Ç.\",\n",
    "    \"–°–±–µ—Ä–µ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Å—á–µ—Ç–∞ –º–æ–≥—É—Ç –±—ã—Ç—å —Å–≤—è–∑–∞–Ω—ã —Å —Ç–µ–∫—É—â–∏–º–∏ —Å—á–µ—Ç–∞–º–∏, —á—Ç–æ–±—ã –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å –∫–æ–º–∏—Å—Å–∏–∏ –∑–∞ –ø–µ—Ä–µ—Ä–∞—Å—Ö–æ–¥.\",\n",
    "    \"–ë–∞–Ω–∫ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —É—Å–ª—É–≥–∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, —á—Ç–æ–±—ã –ø–æ–º–æ—á—å –∫–ª–∏–µ–Ω—Ç–∞–º –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –≤—ã—Ö–æ–¥ –Ω–∞ –ø–µ–Ω—Å–∏—é.\",\n",
    "    \"–£—Å–ª—É–≥–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–ø–ª–∞—Ç—ã —Å—á–µ—Ç–æ–≤ –ø–æ–º–æ–≥–∞—é—Ç –∫–ª–∏–µ–Ω—Ç–∞–º –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–ø—É—Å–∫–∞ —Å—Ä–æ–∫–æ–≤ –æ–ø–ª–∞—Ç—ã –≤–∞–∂–Ω—ã—Ö —Å—á–µ—Ç–æ–≤.\",\n",
    "    \"–û—Ç–¥–µ–ª–µ–Ω–∏—è –±–∞–Ω–∫–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ —è—á–µ–π–∫–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ü–µ–Ω–Ω—ã—Ö –≤–µ—â–µ–π.\",\n",
    "    \"–ö–ª–∏–µ–Ω—Ç—ã –º–æ–≥—É—Ç –ø–æ–¥–∞—Ç—å –∑–∞—è–≤–∫—É –Ω–∞ –ª–∏—á–Ω—ã–µ –∫—Ä–µ–¥–∏—Ç—ã –¥–ª—è –ø–æ–∫—Ä—ã—Ç–∏—è –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω—ã—Ö —Ä–∞—Å—Ö–æ–¥–æ–≤ –∏–ª–∏ –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏ –¥–æ–ª–≥–æ–≤.\",\n",
    "    \"–ò–Ω—Ç–µ—Ä–Ω–µ—Ç-–±–∞–Ω–∫–∏–Ω–≥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ –Ω–∏–∑–∫–∏—Ö –æ—Å—Ç–∞—Ç–∫–∞—Ö –∏–ª–∏ –∫—Ä—É–ø–Ω—ã—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è—Ö.\",\n",
    "    \"–ë–∞–Ω–∫ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã, —á—Ç–æ–±—ã –ø–æ–º–æ—á—å –∫–ª–∏–µ–Ω—Ç–∞–º –ø–æ–Ω—è—Ç—å, –∫–∞–∫ —É–ø—Ä–∞–≤–ª—è—Ç—å —Å–≤–æ–∏–º–∏ –¥–µ–Ω—å–≥–∞–º–∏.\",\n",
    "    \"–ö–ª–∏–µ–Ω—Ç—ã –º–æ–≥—É—Ç –≤—ã–±–∏—Ä–∞—Ç—å –∏–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã—Ö –æ–ø—Ü–∏–π, –≤–∫–ª—é—á–∞—è –∞–∫—Ü–∏–∏ –∏ –æ–±–ª–∏–≥–∞—Ü–∏–∏.\",\n",
    "    \"–£ –±–∞–Ω–∫–∞ –µ—Å—Ç—å –≤—ã–¥–µ–ª–µ–Ω–Ω–∞—è –ª–∏–Ω–∏—è –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–æ–º–æ—â–∏ –≤ –ª—é–±—ã—Ö –≤–æ–ø—Ä–æ—Å–∞—Ö, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å–æ —Å—á–µ—Ç–∞–º–∏.\",\n",
    "    \"–°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—ã –ø–æ –∏–ø–æ—Ç–µ–∫–µ –≥–æ—Ç–æ–≤—ã –ø–æ–º–æ—á—å –ø–æ–∫—É–ø–∞—Ç–µ–ª—è–º –≤–ø–µ—Ä–≤—ã–µ –∫—É–ø–∏—Ç—å –∂–∏–ª—å–µ –ø—Ä–æ–π—Ç–∏ —ç—Ç–æ—Ç –ø—Ä–æ—Ü–µ—Å—Å.\",\n",
    "    \"–°–ª—É–∂–±–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∫—Ä–µ–¥–∏—Ç–æ–≤ –±–∞–Ω–∫–∞ —É–≤–µ–¥–æ–º–ª—è–µ—Ç –∫–ª–∏–µ–Ω—Ç–æ–≤ –æ –ª—é–±—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö –≤ –∏—Ö –∫—Ä–µ–¥–∏—Ç–Ω—ã—Ö –æ—Ç—á–µ—Ç–∞—Ö.\",\n",
    "    \"–ö–ª–∏–µ–Ω—Ç—ã –º–æ–≥—É—Ç –ª–µ–≥–∫–æ –æ–±–Ω–æ–≤–∏—Ç—å —Å–≤–æ—é –∫–æ–Ω—Ç–∞–∫—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —á–µ—Ä–µ–∑ –æ–Ω–ª–∞–π–Ω-–ø–æ—Ä—Ç–∞–ª –±–∞–Ω–∫–∞.\",\n",
    "    \"–°–µ—Ç—å –±–∞–Ω–∫–æ–º–∞—Ç–æ–≤ –±–∞–Ω–∫–∞ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —É–¥–æ–±–Ω—ã–π –¥–æ—Å—Ç—É–ø –∫ —Å–Ω—è—Ç–∏—é –∏ –≤–Ω–µ—Å–µ–Ω–∏—é –Ω–∞–ª–∏—á–Ω—ã—Ö.\",\n",
    "    \"–£—Å–ª—É–≥–∏ –æ–±–º–µ–Ω–∞ –∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω–æ–π –≤–∞–ª—é—Ç—ã –¥–æ—Å—Ç—É–ø–Ω—ã –¥–ª—è –∫–ª–∏–µ–Ω—Ç–æ–≤, –ø–ª–∞–Ω–∏—Ä—É—é—â–∏—Ö –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –ø–æ–µ–∑–¥–∫–∏.\",\n",
    "    \"–°–ª—É–∂–±–∞ –∑–∞—â–∏—Ç—ã –æ—Ç –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞ –±–∞–Ω–∫–∞ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç —Å—á–µ—Ç–∞ –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏.\",\n",
    "    \"–ö–ª–∏–µ–Ω—Ç—ã –º–æ–≥—É—Ç —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –ª–∏–º–∏—Ç—ã —Ä–∞—Å—Ö–æ–¥–æ–≤ –ø–æ —Å–≤–æ–∏–º –∫—Ä–µ–¥–∏—Ç–Ω—ã–º –∫–∞—Ä—Ç–∞–º, —á—Ç–æ–±—ã –ø–æ–º–æ—á—å —É–ø—Ä–∞–≤–ª—è—Ç—å –±—é–¥–∂–µ—Ç–æ–º.\",\n",
    "    \"–ë–∞–Ω–∫ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —Å—Ç—Ä–∞—Ö–æ–≤—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã, –≤–∫–ª—é—á–∞—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–µ, –∞–≤—Ç–æ–º–æ–±–∏–ª—å–Ω–æ–µ –∏ –∂–∏–ª–∏—â–Ω–æ–µ —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ.\",\n",
    "    \"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –º–æ–≥—É—Ç –∑–∞–≥—Ä—É–∂–∞—Ç—å –∏ –ø–µ—á–∞—Ç–∞—Ç—å –µ–∂–µ–º–µ—Å—è—á–Ω—ã–µ –≤—ã–ø–∏—Å–∫–∏ –ø–æ —Å—á–µ—Ç–∞–º –ø—Ä—è–º–æ —Å —Å–∞–π—Ç–∞ –±–∞–Ω–∫–∞.\",\n",
    "    \"–ú–∞–ª—ã–µ –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏—è –º–æ–≥—É—Ç –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ –∫—Ä–µ–¥–∏—Ç–∞–º –∏ –∫—Ä–µ–¥–∏—Ç–Ω—ã–º –ª–∏–Ω–∏—è–º –¥–ª—è —Ä–æ—Å—Ç–∞ —Å–≤–æ–∏—Ö –±–∏–∑–Ω–µ—Å–æ–≤.\",\n",
    "    \"–ë–∞–Ω–∫ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –æ–Ω–ª–∞–π–Ω-–æ–ø—Ü–∏–∏ –æ–ø–ª–∞—Ç—ã –¥–ª—è –ø–æ–∫—É–ø–æ–∫ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π –∫–æ–º–º–µ—Ä—Ü–∏–∏.\",\n",
    "    \"–ú–æ–±–∏–ª—å–Ω—ã–µ –±–∞–Ω–∫–æ–≤—Å–∫–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –ø–æ–∑–≤–æ–ª—è—é—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –ø—Ä–æ–≤–µ—Ä—è—Ç—å —Å–≤–æ–∏ –±–∞–ª–∞–Ω—Å—ã —Å—á–µ—Ç–æ–≤ –Ω–∞ —Ö–æ–¥—É.\",\n",
    "    \"–ü–µ–Ω—Å–∏–æ–Ω–Ω—ã–µ —Å—á–µ—Ç–∞ –±–∞–Ω–∫–∞ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–∞–ª–æ–≥–æ–≤—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞, —á—Ç–æ–±—ã –ø–æ–º–æ—á—å –∫–ª–∏–µ–Ω—Ç–∞–º —Å—ç–∫–æ–Ω–æ–º–∏—Ç—å –Ω–∞ –±—É–¥—É—â–µ–µ.\",\n",
    "    \"–§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç—ã –±–∞–Ω–∫–∞ –º–æ–≥—É—Ç –ø–æ–º–æ—á—å –∫–ª–∏–µ–Ω—Ç–∞–º —Ä–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏.\",\n",
    "    \"–ë–∞–Ω–∫ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∫—Ä–µ–¥–∏—Ç—ã —Å –Ω–∏–∑–∫–∏–º–∏ –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–º–∏ —Å—Ç–∞–≤–∫–∞–º–∏ –¥–ª—è –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏ –¥—Ä—É–≥–∏—Ö –∫—Ä—É–ø–Ω—ã—Ö –∂–∏–∑–Ω–µ–Ω–Ω—ã—Ö —Ä–∞—Å—Ö–æ–¥–æ–≤.\",\n",
    "    \"–ö–ª–∏–µ–Ω—Ç—ã –º–æ–≥—É—Ç –æ—Å—É—â–µ—Å—Ç–≤–ª—è—Ç—å –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –¥–µ–Ω–µ–∂–Ω—ã–µ –ø–µ—Ä–µ–≤–æ–¥—ã –ø–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã–º –æ–±–º–µ–Ω–Ω—ã–º –∫—É—Ä—Å–∞–º.\",\n",
    "    \"–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ–Ω–ª–∞–π–Ω-–ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –±–∞–Ω–∫–∞ –∑–∞—â–∏—â–∞–µ—Ç –ª–∏—á–Ω—É—é –∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∫–ª–∏–µ–Ω—Ç–æ–≤.\",\n",
    "    \"–ö—Ä–µ–¥–∏—Ç–Ω—ã–µ –ª–∏–Ω–∏–∏ –ø–æ–¥ –∑–∞–ª–æ–≥ –¥–æ–º–∞ –¥–æ—Å—Ç—É–ø–Ω—ã –¥–ª—è –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤ –¥–æ–º–æ–≤, –Ω—É–∂–¥–∞—é—â–∏—Ö—Å—è –≤ –¥–æ—Å—Ç—É–ø–µ –∫ –Ω–∞–ª–∏—á–Ω—ã–º.\",\n",
    "    \"–ë–∞–Ω–∫ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –¥–µ–ø–æ–∑–∏—Ç—ã —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Å—Ä–æ–∫–æ–º —Å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–º–∏ –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–º–∏ —Å—Ç–∞–≤–∫–∞–º–∏ –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö —Å–±–µ—Ä–µ–∂–µ–Ω–∏–π.\",\n",
    "    \"–ö–ª–∏–µ–Ω—Ç—ã –º–æ–≥—É—Ç –ø–æ–ª—É—á–∞—Ç—å –¥–æ—Å—Ç—É–ø –∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —Å—á–µ—Ç—É –∫—Ä—É–≥–ª–æ—Å—É—Ç–æ—á–Ω–æ —á–µ—Ä–µ–∑ –º–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –±–∞–Ω–∫–∞.\",\n",
    "    \"–ü—Ä–æ–≥—Ä–∞–º–º—ã –ª–æ—è–ª—å–Ω–æ—Å—Ç–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤ –±–∞–Ω–∫–∞ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ç–∞–∫–∏–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞, –∫–∞–∫ —Å–Ω–∏–∂–µ–Ω–Ω—ã–µ –∫–æ–º–∏—Å—Å–∏–∏ –∏ –ø–æ–≤—ã—à–µ–Ω–Ω—ã–µ –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–µ —Å—Ç–∞–≤–∫–∏.\",\n",
    "    \"–†–æ–¥–∏—Ç–µ–ª–∏ –º–æ–≥—É—Ç –æ—Ç–∫—Ä—ã—Ç—å —Å–±–µ—Ä–µ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Å—á–µ—Ç–∞ –¥–ª—è —Å–≤–æ–∏—Ö –¥–µ—Ç–µ–π, —á—Ç–æ–±—ã –Ω–∞—É—á–∏—Ç—å –∏—Ö —É–ø—Ä–∞–≤–ª–µ–Ω–∏—é –¥–µ–Ω—å–≥–∞–º–∏.\",\n",
    "    \"–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –±—é–¥–∂–µ—Ç–∞ –±–∞–Ω–∫–∞ –ø–æ–º–æ–≥–∞—é—Ç –∫–ª–∏–µ–Ω—Ç–∞–º –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å —Å–≤–æ–∏ —Ä–∞—Å—Ö–æ–¥—ã –∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Ü–µ–ª–∏.\",\n",
    "    \"–ö–ª–∏–µ–Ω—Ç—ã –º–æ–≥—É—Ç –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è –¥–ª—è –ø—Ä—è–º–æ–≥–æ –¥–µ–ø–æ–∑–∏—Ç–∞, —á—Ç–æ–±—ã –∏—Ö –∑–∞—Ä–ø–ª–∞—Ç—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞—á–∏—Å–ª—è–ª–∏—Å—å –Ω–∞ —Å—á–µ—Ç.\",\n",
    "    \"–ë–∞–Ω–∫ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å—á–µ—Ç–∞ –¥–ª—è —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ –±–µ–∑ –µ–∂–µ–º–µ—Å—è—á–Ω—ã—Ö –∫–æ–º–∏—Å—Å–∏–π –∏ —Å –Ω–∏–∑–∫–∏–º–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –æ—Å—Ç–∞—Ç–∫–∞–º–∏.\",\n",
    "    \"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –º–æ–≥—É—Ç –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –±—É–¥—É—â–∏–µ –ø–ª–∞—Ç–µ–∂–∏ –∏ –ø–µ—Ä–µ–≤–æ–¥—ã, –∏—Å–ø–æ–ª—å–∑—É—è —É—Å–ª—É–≥–∏ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–±–∞–Ω–∫–∏–Ω–≥–∞ –±–∞–Ω–∫–∞.\",\n",
    "    \"–ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã –±–∞–Ω–∫–∞ –≤–∫–ª—é—á–∞—é—Ç –≤–∑–∞–∏–º–Ω—ã–µ —Ñ–æ–Ω–¥—ã –∏ –ø–µ–Ω—Å–∏–æ–Ω–Ω—ã–µ —Å—á–µ—Ç–∞.\",\n",
    "    \"–ö–ª–∏–µ–Ω—Ç—ã –º–æ–≥—É—Ç –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ —É—Å–ª—É–≥–∞–º —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å–≤–æ–∏–º–∏ –∞–∫—Ç–∏–≤–∞–º–∏ –∏ –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞–º–∏.\",\n",
    "    \"–ë–∞–Ω–∫ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∫—Ä–µ–¥–∏—Ç–Ω—ã–µ –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä—ã, —á—Ç–æ–±—ã –ø–æ–º–æ—á—å –∫–ª–∏–µ–Ω—Ç–∞–º –æ—Ü–µ–Ω–∏—Ç—å –∏—Ö –µ–∂–µ–º–µ—Å—è—á–Ω—ã–µ –ø–ª–∞—Ç–µ–∂–∏.\",\n",
    "    \"–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –æ–±–º–µ–Ω —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏ —á–µ—Ä–µ–∑ –æ–Ω–ª–∞–π–Ω-–ø–ª–∞—Ç—Ñ–æ—Ä–º—É –±–∞–Ω–∫–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–ª–∏–µ–Ω—Ç–∞–º –æ–±—â–∞—Ç—å—Å—è —Å–æ —Å–≤–æ–∏–º–∏ –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç–∞–º–∏.\",\n",
    "    \"–§—É–Ω–∫—Ü–∏—è –º–æ–±–∏–ª—å–Ω–æ–≥–æ –¥–µ–ø–æ–∑–∏—Ç–∞ —á–µ–∫–æ–≤ –±–∞–Ω–∫–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ª–µ–≥–∫–æ –≤–Ω–æ—Å–∏—Ç—å —á–µ–∫–∏ –±–µ–∑ –ø–æ—Å–µ—â–µ–Ω–∏—è –æ—Ç–¥–µ–ª–µ–Ω–∏—è.\",\n",
    "    \"–ö–ª–∏–µ–Ω—Ç—ã –º–æ–≥—É—Ç –∑–∞—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –±–∞–ª–ª—ã –∑–∞ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –∑–∞ –∫–∞–∂–¥—ã–π –ø–æ—Ç—Ä–∞—á–µ–Ω–Ω—ã–π –¥–æ–ª–ª–∞—Ä –Ω–∞ —Å–≤–æ–∏—Ö –∫—Ä–µ–¥–∏—Ç–Ω—ã—Ö –∫–∞—Ä—Ç–∞—Ö.\",\n",
    "    \"–ë–∞–Ω–∫ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Å–µ–º–∏–Ω–∞—Ä—ã –ø–æ –ª–∏—á–Ω—ã–º —Ñ–∏–Ω–∞–Ω—Å–∞–º, —á—Ç–æ–±—ã –æ–±—É—á–∏—Ç—å –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—é –±—é–¥–∂–µ—Ç–∞ –∏ —Å–±–µ—Ä–µ–∂–µ–Ω–∏—è–º.\",\n",
    "    \"–ö–ª–∏–µ–Ω—Ç—ã –º–æ–≥—É—Ç –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å —Å–≤–æ–∏ –ø—Ä–∏–≤—ã—á–∫–∏ —Ç—Ä–∞—Ç —Å –ø–æ–º–æ—â—å—é –æ–Ω–ª–∞–π–Ω-–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Ä–∞—Å—Ö–æ–¥–æ–≤ –±–∞–Ω–∫–∞.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "931f1e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    for i in text:\n",
    "        if i in exclude:\n",
    "            text = text.replace(i, ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "dcc00148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langdetect \n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "22d7d826",
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_stemmer = SnowballStemmer(language='russian')\n",
    "en_stemmer =  SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "20c6631d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "b0430a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(text):\n",
    "    language_detected = detect(text)\n",
    "    if language_detected =='ru':\n",
    "        text = text.lower()\n",
    "        text = remove_punctuation(text)\n",
    "        text = [i.text for i in tokenize(text) if i.text not in stop_words_ru]\n",
    "        text = [ru_stemmer.stem(i) for i in text]\n",
    "    else:\n",
    "        text = text.lower()\n",
    "        text = remove_punctuation(text)\n",
    "        text = [i.text for i in tokenize(text) if i.text not in stop_words_en]\n",
    "        text = [en_stemmer.stem(i) for i in text]\n",
    "    return text\n",
    "\n",
    "text_list = [preprocess(i) for i in banking_sentences]\n",
    "len(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "fbbc1600-d4ce-45b3-a551-6738f57257f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The bank offers a variety of savings accounts with different interest rates to suit individual needs.', 0.5981308411214953, 0)\n",
      "('Parents can open savings accounts for their children to teach them about money management.', 0.5274725274725275, 38)\n",
      "('Loans for purchasing homes are available with fixed or variable interest rates.', 0.5176470588235295, 5)\n",
      "('The bank offers fixed-term deposits with higher interest rates for long-term savings.', 0.5050505050505051, 35)\n",
      "('Savings accounts can be linked to checking accounts to prevent overdraft fees.', 0.4731182795698925, 8)\n"
     ]
    }
   ],
   "source": [
    "def search(query):\n",
    "    query = ' '.join(preprocess(query))\n",
    "    results = []\n",
    "    for i in text_list:\n",
    "        textss = ' '.join(i)\n",
    "        sm=difflib.SequenceMatcher(a=query, b=textss)\n",
    "        idx = text_list.index(i)\n",
    "        results.append((banking_sentences[idx], sm.ratio(), idx))\n",
    "    max_index = sorted(results, key = lambda x: x[1], reverse=True)\n",
    "    return max_index[:5]\n",
    "\n",
    "# Example usage\n",
    "user_query = \"I need saving account with high interest rate\"\n",
    "res = search(user_query)\n",
    "for i in res:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "be55ad93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('–ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã –±–∞–Ω–∫–∞ –≤–∫–ª—é—á–∞—é—Ç –≤–∑–∞–∏–º–Ω—ã–µ —Ñ–æ–Ω–¥—ã –∏ –ø–µ–Ω—Å–∏–æ–Ω–Ω—ã–µ —Å—á–µ—Ç–∞.', 0.35294117647058826, 94)\n",
      "('–ü–µ–Ω—Å–∏–æ–Ω–Ω—ã–µ —Å—á–µ—Ç–∞ –±–∞–Ω–∫–∞ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–∞–ª–æ–≥–æ–≤—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞, —á—Ç–æ–±—ã –ø–æ–º–æ—á—å –∫–ª–∏–µ–Ω—Ç–∞–º —Å—ç–∫–æ–Ω–æ–º–∏—Ç—å –Ω–∞ –±—É–¥—É—â–µ–µ.', 0.2823529411764706, 80)\n",
      "('–£ –±–∞–Ω–∫–∞ –µ—Å—Ç—å –≤—ã–¥–µ–ª–µ–Ω–Ω–∞—è –ª–∏–Ω–∏—è –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–æ–º–æ—â–∏ –≤ –ª—é–±—ã—Ö –≤–æ–ø—Ä–æ—Å–∞—Ö, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å–æ —Å—á–µ—Ç–∞–º–∏.', 0.2702702702702703, 67)\n",
      "('–ö–ª–∏–µ–Ω—Ç—ã –º–æ–≥—É—Ç –ª–µ–≥–∫–æ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å –¥–µ–Ω—å–≥–∏ –º–µ–∂–¥—É —Å–≤–æ–∏–º–∏ —Å—á–µ—Ç–∞–º–∏ —Å –ø–æ–º–æ—â—å—é –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–±–∞–Ω–∫–∏–Ω–≥–∞.', 0.2564102564102564, 52)\n",
      "('–ö–ª–∏–µ–Ω—Ç—ã –º–æ–≥—É—Ç –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è –¥–ª—è –ø—Ä—è–º–æ–≥–æ –¥–µ–ø–æ–∑–∏—Ç–∞, —á—Ç–æ–±—ã –∏—Ö –∑–∞—Ä–ø–ª–∞—Ç—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞—á–∏—Å–ª—è–ª–∏—Å—å –Ω–∞ —Å—á–µ—Ç.', 0.25, 91)\n"
     ]
    }
   ],
   "source": [
    "user_query = \"–ø–µ–Ω—Å–∏–æ–Ω–Ω—ã–π —Å—á–µ—Ç\"\n",
    "res = search(user_query)\n",
    "for i in res:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bf9e75-201a-499d-b223-3fe733926cef",
   "metadata": {},
   "source": [
    "<font size=\"5\">3. Text Cleaning by Removing Blacklist Words or Phrases</font>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "e06efe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "26ff5143",
   "metadata": {},
   "outputs": [],
   "source": [
    "badword_base = ['nigga', 'nigger', 'faggot', '–≥–µ–π', '–¢–æ–∫–∞–µ–≤', '–ü—É—Ç–∏–Ω', 'uzbek']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "36447322",
   "metadata": {},
   "outputs": [],
   "source": [
    "badwords = []\n",
    "for i in badword_base:\n",
    "    badwords.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "08ed3a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_word(word):\n",
    "    bigram_word_array = []\n",
    "    for i in range(len(word) - 1):\n",
    "        bigram_word_array.append(word[i: i + 2])\n",
    "    return bigram_word_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "e10d2b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_text(text):\n",
    "    tokenized = text.split()\n",
    "    bigram_text_array = [] \n",
    "    for i in range(len(tokenized) - 1):\n",
    "        bigram_text_array.append(' '.join(tokenized[i:i + 2]))\n",
    "    return text.split() + bigram_text_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "b7d03223",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramed_words = [bigram_word(i.lower()) for i in badwords]\n",
    "bigram_corpus_set = set(sum(bigramed_words,[]))\n",
    "bigrams_indexed = {}\n",
    "for bigram in bigram_corpus_set:\n",
    "    for index,bigrammed_word in enumerate(bigramed_words):\n",
    "        if bigram in bigrammed_word:\n",
    "            try:\n",
    "                bigrams_indexed[bigram].append(index)\n",
    "            except:\n",
    "                bigrams_indexed[bigram] = []\n",
    "                bigrams_indexed[bigram].append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "62596f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_bigram = {}\n",
    "for i in range(len(badwords)):\n",
    "    word_bigram[badwords[i]] = bigramed_words[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "dc46981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bigram(text): \n",
    "    results = []\n",
    "    for word in bigram_text(text):\n",
    "        idx = []\n",
    "        not_found = []\n",
    "        for bigram in bigram_word(word):\n",
    "            try: \n",
    "                idx.extend(bigrams_indexed[bigram])\n",
    "            except:\n",
    "                not_found.append(bigram)\n",
    "        if len(idx):\n",
    "            counted_numbers = Counter(idx)\n",
    "            sorted_numbers = sorted(counted_numbers.items(), key=lambda x: x[1], reverse=True)\n",
    "            coff = (sorted_numbers[0][1])/(len(word_bigram[badwords[sorted_numbers[0][0]]]) + len(not_found))\n",
    "            results.append((word, sorted_numbers, coff))\n",
    "    max_index = max(range(len(results)), key=lambda i: results[i][2])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "c5010f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_thresold(results, threshold):\n",
    "    res = []\n",
    "    results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "    for word, index, prob in results:\n",
    "        if float(prob) >= float(threshold):\n",
    "            res.append((word, badwords[index[0][0]]))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "edafe251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('–≥–µ–π', '–≥–µ–π'), ('niggas', 'nigga'), ('–¢–æ–∫–∞–µ–≤', '–¢–æ–∫–∞–µ–≤'), ('uzber.', 'uzbek'), ('is niggas', 'nigga'), ('niggas and', 'nigga'), ('and –¢–æ–∫–∞–µ–≤', '–¢–æ–∫–∞–µ–≤'), ('–¢–æ–∫–∞–µ–≤ –¢–∏–≥—Ä', '–¢–æ–∫–∞–µ–≤'), ('and –≥–µ–π', '–≥–µ–π'), ('–≥–µ–π and', '–≥–µ–π'), ('–¢–∏–≥—Ä uzber.', 'uzbek')]\n"
     ]
    }
   ],
   "source": [
    "query = 'Here is a sample text including the word –ü—É—Ç–µ–Ω is niggas and –≥–µ–π and –¢–æ–∫–∞–µ–≤ –¢–∏–≥—Ä uzber.' \n",
    "results = find_bigram(query)\n",
    "valid_badword = []\n",
    "res = take_thresold(results, 0.2)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "995f7c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a sample text including the word –ü—É—Ç–µ–Ω is *** and *** and *** –¢–∏–≥—Ä ***\n"
     ]
    }
   ],
   "source": [
    "for word, _ in res:\n",
    "    query = query.replace(word, '***')\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "d5a34dff-6233-4ef9-866e-5dd21046049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(query):\n",
    "    results = find_bigram(query)\n",
    "    valid_badword = []\n",
    "    res = take_thresold(results, 0.2)\n",
    "    for word, _ in res:\n",
    "        query = query.replace(word, '***')\n",
    "\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "d002948d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you black ***'"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text('you black nigger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18656c02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
